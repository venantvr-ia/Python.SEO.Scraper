# SEO Scraper Service

Micro-service FastAPI de scraping haute performance avec [Crawl4AI](https://github.com/unclecode/crawl4ai).

## ğŸ“‹ FonctionnalitÃ©s

- ğŸš€ Scraping ultra-rapide avec Crawl4AI et Playwright
- ğŸ¯ API REST simple et intuitive
- ğŸ“ Export en Markdown nettoyÃ©
- âš¡ Support du scraping parallÃ¨le (batch)
- ğŸ”’ Gestion robuste des erreurs et timeouts
- ğŸ“Š Health check intÃ©grÃ©
- ğŸ³ PrÃªt pour Docker
- ğŸ“š Documentation Swagger automatique

## ğŸ—ï¸ Structure du projet

```
Python.SEO.Scraper/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ seo_scraper/
â”‚       â”œâ”€â”€ __init__.py       # Exports publics
â”‚       â”œâ”€â”€ __main__.py       # Point d'entrÃ©e CLI
â”‚       â”œâ”€â”€ api.py            # Endpoints FastAPI
â”‚       â”œâ”€â”€ config.py         # Configuration centralisÃ©e
â”‚       â”œâ”€â”€ models.py         # ModÃ¨les Pydantic
â”‚       â””â”€â”€ scraper.py        # Service de scraping
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ samples/              # Fichiers MD de test
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py           # Configuration pytest
â”‚   â”œâ”€â”€ test_api.py           # Tests API
â”‚   â””â”€â”€ test_models.py        # Tests modÃ¨les
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ test_scrape.py        # Script de test fonctionnel
â”œâ”€â”€ Dockerfile                # Image Docker
â”œâ”€â”€ docker-compose.yml        # Orchestration Docker
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env.example              # Exemple de configuration
â”œâ”€â”€ pyproject.toml            # Configuration du projet
â”œâ”€â”€ Makefile                  # Commandes de dÃ©veloppement
â””â”€â”€ README.md
```

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10+
- pip

### Installation rapide

```bash
# Cloner le dÃ©pÃ´t
git clone <repo-url>
cd Python.SEO.Scraper

# Installer les dÃ©pendances
make install

# Ou pour le dÃ©veloppement
make install-dev
```

### Configuration

Copier le fichier d'exemple et l'adapter si nÃ©cessaire :

```bash
cp .env.example .env
```

Variables d'environnement disponibles :

| Variable           | DÃ©faut    | Description                                 |
|--------------------|-----------|---------------------------------------------|
| `HOST`             | `0.0.0.0` | Adresse d'Ã©coute du serveur                 |
| `PORT`             | `8001`    | Port d'Ã©coute                               |
| `LOG_LEVEL`        | `INFO`    | Niveau de log (DEBUG, INFO, WARNING, ERROR) |
| `CRAWLER_HEADLESS` | `true`    | Mode headless pour le navigateur            |
| `DEFAULT_TIMEOUT`  | `30000`   | Timeout par dÃ©faut (ms)                     |

## ğŸ¯ Utilisation

### Lancement du service

```bash
# Mode production
make run

# Mode dÃ©veloppement (avec auto-reload)
make run-dev

# Ou directement avec le CLI installÃ©
seo-scraper
```

Le service dÃ©marre sur `http://localhost:8001`

Documentation interactive : `http://localhost:8001/docs`

### API Endpoints

#### `GET /health`

VÃ©rifie l'Ã©tat du service.

```bash
curl http://localhost:8001/health
```

RÃ©ponse :

```json
{
  "status": "healthy",
  "crawler_ready": true,
  "version": "1.0.0"
}
```

#### `POST /scrape`

Scrape une URL et retourne le contenu en Markdown.

```bash
curl -X POST http://localhost:8001/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "timeout": 30000
  }'
```

**Request:**

```json
{
  "url": "https://example.com",
  "ignore_body_visibility": true,
  "timeout": 30000
}
```

**Response:**

```json
{
  "url": "https://example.com",
  "success": true,
  "markdown": "# Example Domain\n\nThis domain is for use...",
  "content_length": 1234,
  "error": null
}
```

#### `POST /scrape/batch`

Scrape plusieurs URLs en parallÃ¨le.

```bash
curl -X POST http://localhost:8001/scrape/batch \
  -H "Content-Type: application/json" \
  -d '["https://example.com", "https://example.org"]'
```

**Response:** Array de `ScrapeResponse`

### Utilisation en Python

```python
import httpx

# Client pour le service
client = httpx.Client(base_url="http://localhost:8001")

# Scraper une URL
response = client.post("/scrape", json={
    "url": "https://example.com",
    "timeout": 60000
})
data = response.json()

if data["success"]:
    print(f"Contenu: {data['markdown'][:100]}...")
else:
    print(f"Erreur: {data['error']}")
```

## ğŸ§ª Tests

```bash
# ExÃ©cuter tous les tests
make test

# Tests avec couverture
make test-cov

# Rapport HTML de couverture gÃ©nÃ©rÃ© dans htmlcov/
```

## ğŸ› ï¸ DÃ©veloppement

### Commandes disponibles

```bash
# Installation
make install           # Installation production
make install-dev       # Installation dÃ©veloppement

# Lancement
make run               # Lancer en production
make run-dev           # Lancer en mode dev (auto-reload)

# Tests & QualitÃ©
make test              # ExÃ©cuter les tests
make test-cov          # Tests avec couverture
make lint              # VÃ©rifier le code (ruff)
make format            # Formater le code (black)
make check-format      # VÃ©rifier le formatage

# Scraping
make scrape            # Test de scraping (URL=... optionnel)
make scrape-save       # Scrape et sauvegarde dans tests/samples/

# Utilitaires
make clean             # Nettoyer les fichiers temp
make clean-all         # Nettoyage complet (+ venv)
make check             # VÃ©rifier si le service tourne
make status            # Afficher le statut dÃ©taillÃ©

# Docker
make docker-build      # Construire l'image Docker
make docker-run        # Lancer avec docker compose
make docker-stop       # ArrÃªter le conteneur
make docker-logs       # Voir les logs
```

### QualitÃ© du code

Le projet utilise :

- **black** pour le formatage
- **ruff** pour le linting
- **pytest** pour les tests

```bash
# Formatter automatiquement
make format

# VÃ©rifier sans modifier
make lint
make check-format
```

## ğŸ³ Docker

### Avec Docker Compose (recommandÃ©)

```bash
# Construire et lancer
docker compose up -d

# Voir les logs
docker compose logs -f

# ArrÃªter
docker compose down
```

### Avec Docker directement

```bash
# Construire l'image
docker build -t seo-scraper .

# Lancer le conteneur
docker run -d \
  --name seo-scraper \
  -p 8001:8001 \
  -e LOG_LEVEL=INFO \
  -e DEFAULT_TIMEOUT=30000 \
  seo-scraper

# VÃ©rifier le statut
docker logs seo-scraper
curl http://localhost:8001/health
```

### Configuration Docker

Variables d'environnement disponibles :

```yaml
environment:
  - HOST=0.0.0.0
  - PORT=8001
  - LOG_LEVEL=INFO
  - CRAWLER_HEADLESS=true
  - DEFAULT_TIMEOUT=30000
```

### Health Check

Le conteneur inclut un health check automatique qui vÃ©rifie `/health` toutes les 30 secondes.

## ğŸ”— IntÃ©gration avec Python.SEO.Gemini

Ce micro-service est conÃ§u pour Ãªtre utilisÃ© avec Python.SEO.Gemini :

```python
# Dans le .env de Python.SEO.Gemini
SCRAPER_SERVICE_URL = "http://localhost:8001"
SCRAPER_TIMEOUT = 60
```

## ğŸ“ Licence

MIT

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une PR.

## ğŸ“ Support

Pour toute question ou problÃ¨me, ouvrez une issue sur GitHub.
